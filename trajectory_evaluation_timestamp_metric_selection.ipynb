{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine metrics for timestamped trajectory evaluation\n",
    "\n",
    "For trajectory information, we have spatial ground truth but **no** temporal ground truth. This is not surprising since the time taken to travel the same route on different days can be different based on the traffic conditions, train delays, etc. We capture high quality (high accuracy/high frequency/always on) temporal data from two different phone during the travel, and mark the temporal ground truth for the start and end of travel segments (trip/section) on the phone.\n",
    "\n",
    "In this notebook, we explore the various options to create a _reference_ trajectory using the spatial ground truth and the timestamped accuracy control trajectories from android and iOS. We can then use the reference trajectory to evaluate the spatio-temporal accuracy of the experiment trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reading and validating data\n",
    "import emeval.input.spec_details as eisd\n",
    "import emeval.input.phone_view as eipv\n",
    "import emeval.input.eval_view as eiev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization helpers\n",
    "import emeval.viz.phone_view as ezpv\n",
    "import emeval.viz.eval_view as ezev\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emeval.metrics.reference_trajectory as emr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For computation\n",
    "import numpy as np\n",
    "import math\n",
    "import functools\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import shapely as shp\n",
    "import arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import branca.element as bre\n",
    "import geojson as gj\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anytree as at\n",
    "import anytree.importer as ati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASTORE_URL = \"http://cardshark.cs.berkeley.edu\"\n",
    "AUTHOR_EMAIL = \"shankari@eecs.berkeley.edu\"\n",
    "sd_la = eisd.SpecDetails(DATASTORE_URL, AUTHOR_EMAIL, \"unimodal_trip_car_bike_mtv_la\")\n",
    "sd_sj = eisd.SpecDetails(DATASTORE_URL, AUTHOR_EMAIL, \"car_scooter_brex_san_jose\")\n",
    "sd_ucb = eisd.SpecDetails(DATASTORE_URL, AUTHOR_EMAIL, \"train_bus_ebike_mtv_ucb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(ezpv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_la = eipv.PhoneView(sd_la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_sj = eipv.PhoneView(sd_sj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_ucb = eipv.PhoneView(sd_ucb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test trajectories\n",
    "\n",
    "We select four different runs as our deep-dive dataset to explore various reference trajectory creation options. These are:\n",
    "1. `easiest`: overground, unimodal trip by car and bike where both control phones had very little error\n",
    "1. `weird_android`: different run of trip (1) where the android control phone had very poor quality data even though we requested high quality data\n",
    "1. `temporal_zigzags`: this trip is really the motivation for spatio-temporal accuracy. It is an underground trip with zig zags a prior point along the trip, classic example of why spatio-temporal accuracy is needed\n",
    "1. `backtracking`: trip that includes a short u-turn + backtrack to ensure that real zigzags are respected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_dive_dataset = {\n",
    "    \"easiest\": {\n",
    "        \"ground_truth\": {\n",
    "            \"leg\": sd_la.get_ground_truth_for_leg(\"suburb_city_driving_weekend\", \"suburb_city_driving_weekend\")\n",
    "        },\n",
    "        \"temporal_control\": {\n",
    "            \"android\": pv_la.map()[\"android\"][\"ucb-sdb-android-1\"][\"evaluation_ranges\"][1][\"evaluation_trip_ranges\"][0],\n",
    "            \"ios\": pv_la.map()[\"ios\"][\"ucb-sdb-ios-1\"][\"evaluation_ranges\"][1][\"evaluation_trip_ranges\"][0]\n",
    "        }\n",
    "    },\n",
    "    \"weird_android\": {\n",
    "        \"ground_truth\": {\n",
    "            \"leg\": sd_la.get_ground_truth_for_leg(\"suburb_city_driving_weekend\", \"suburb_city_driving_weekend\")\n",
    "        },\n",
    "        \"temporal_control\": {\n",
    "            \"android\": pv_la.map()[\"android\"][\"ucb-sdb-android-1\"][\"evaluation_ranges\"][0][\"evaluation_trip_ranges\"][0],\n",
    "            \"ios\": pv_la.map()[\"ios\"][\"ucb-sdb-ios-1\"][\"evaluation_ranges\"][0][\"evaluation_trip_ranges\"][0]\n",
    "        }\n",
    "    },\n",
    "    \"temporal_zigzags\": {\n",
    "        \"ground_truth\": {\n",
    "            \"leg\": sd_ucb.get_ground_truth_for_leg(\"mtv_to_berkeley_sf_bart\", \"subway_underground\")\n",
    "        },\n",
    "        \"temporal_control\": {\n",
    "            \"android\": pv_ucb.map()[\"android\"][\"ucb-sdb-android-1\"][\"evaluation_ranges\"][0][\"evaluation_trip_ranges\"][0][\"evaluation_section_ranges\"][5],\n",
    "            \"ios\": pv_ucb.map()[\"ios\"][\"ucb-sdb-ios-1\"][\"evaluation_ranges\"][0][\"evaluation_trip_ranges\"][0][\"evaluation_section_ranges\"][5]\n",
    "        }\n",
    "    },\n",
    "    \"backtracking\": {\n",
    "        \"ground_truth\": {\n",
    "            \"leg\": sd_sj.get_ground_truth_for_leg(\"bus trip with e-scooter access\", \"city_escooter\")            \n",
    "        },\n",
    "        \"temporal_control\": {\n",
    "            \"android\": pv_sj.map()[\"android\"][\"ucb-sdb-android-1\"][\"evaluation_ranges\"][0][\"evaluation_trip_ranges\"][1][\"evaluation_section_ranges\"][1],\n",
    "            \"ios\": pv_sj.map()[\"ios\"][\"ucb-sdb-ios-1\"][\"evaluation_ranges\"][0][\"evaluation_trip_ranges\"][1][\"evaluation_section_ranges\"][1]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(eisd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_section_and_points(curr_map, eval_section, loc_df_label, layer_name, disp_color, with_points=False):\n",
    "    sensed_location_df = eval_section[loc_df_label]\n",
    "    print(\"Adding section for %s with length %s\" % (layer_name, len(sensed_location_df)))\n",
    "    sensed_section_gj = gj.Feature(geometry=gj.LineString(coordinates=list(zip(sensed_location_df.longitude, sensed_location_df.latitude))),\n",
    "                                   properties={\"style\": {\"color\": disp_color}, \"ts\": list(sensed_location_df.ts)})\n",
    "    sensed_leg_gj_feature = folium.GeoJson(sensed_section_gj, name=\"sensed_values (%s)\" % layer_name)\n",
    "    curr_map.add_child(sensed_leg_gj_feature)\n",
    "    if with_points:\n",
    "        sensed_leg_gj_points = ezpv.get_point_markers(sensed_section_gj, name=\"sensed_points (%s)\" % layer_name,\n",
    "                                                      color=disp_color, tz=\"America/Los_Angeles\")\n",
    "        curr_map.add_child(sensed_leg_gj_points)\n",
    "\n",
    "def display_gt_and_controls(entry, loc_df_label, with_points=False):\n",
    "    curr_map = folium.Map()\n",
    "    print(\"Using ground truth %s\" % entry[\"ground_truth\"][\"leg\"][\"id\"])\n",
    "    gt_leg_gj = eisd.SpecDetails.get_geojson_for_leg(entry[\"ground_truth\"][\"leg\"])\n",
    "    gt_leg_gj_feature = folium.GeoJson(gt_leg_gj, name=\"ground_truth\")\n",
    "    curr_map.add_child(gt_leg_gj_feature)\n",
    "    if with_points:\n",
    "        gt_leg_gj_points = ezpv.get_point_markers(gt_leg_gj[2], name=\"ground_truth_points\", color=\"green\")\n",
    "        curr_map.add_child(gt_leg_gj_points)\n",
    "    \n",
    "    add_section_and_points(curr_map, entry[\"temporal_control\"][\"android\"], loc_df_label, \"android\", \"orange\", with_points)\n",
    "    add_section_and_points(curr_map, entry[\"temporal_control\"][\"ios\"], loc_df_label, \"ios\", \"purple\", with_points)\n",
    "    \n",
    "    curr_map.fit_bounds(gt_leg_gj_feature.get_bounds())\n",
    "    folium.LayerControl().add_to(curr_map)\n",
    "    return curr_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_list = [display_gt_and_controls(e, \"location_df\") for e in deep_dive_dataset.values()]; len(map_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_maps = bre.Figure()\n",
    "for i, curr_map in enumerate(map_list):\n",
    "    evaluation_maps.add_subplot(2, 2, i+1).add_child(curr_map)\n",
    "evaluation_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features of interest in the maps:\n",
    "- `easiest`: android and ios match beautifully with ground truth\n",
    "- `weird_android`: ios matches with ground truth, but android (orange) has inconsistencies near the start (triangle) and deviates a couple of other times in the middle\n",
    "- `temporal_zigzags`: the ios data is much less accurate, with loops around San Bruno and Daly City and a mid-bay zigzag. However, it is much more complete, extending almost all the way to Berkeley. The android trajectory seems to be closer to the ground truth, but abruptly ends around Duboce/Mission and does not continue during this section\n",
    "- `backtracking`: intentional backtracking at the foot of Coleman Ave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference trajectory creation components\n",
    "\n",
    "Given these errors in the control trajectories, we now consider multiple potential algorithms for converting them to reference trajectories. These algorithms represent a tradeoff between correctness and usability. For example, we could choose a really conservative algorithm which would be very likely to be close to ground truth, but only few runs might result in usable reference trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "Before we compare temporal trajectories, we need to normalize them. This is because the control data is sensed from two separate phones, and the timestamps are different. So we need to align the timestamp trajectories so that we can see how far apart they are. We use a simple normalization in which we linearly interpolate the lat/lng separately at a set of common timestamps. \n",
    "\n",
    "**Note**: we have to restrict the common timestamps to the range of input locations for each trajectory. This is because interpolation does not work well, and in cases where the trajectory is incomplete (`temporal_zigzags`) the generated location for the entire missing timestamp range is the final point. We ensure that the timestamps across phone still align by converting them to ints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distances and projections\n",
    "\n",
    "In some of the more complex comparisons, we will distance comparisons between trajectories. We use two kinds of potential distances:\n",
    "1. `projection`: which is the projection of one trajectory on the other\n",
    "1. `distance`: which is the distance between the two trajectories at a particular point\n",
    "\n",
    "The comparisons can be between the control trajectories, or between the control trajectories and ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering\n",
    "\n",
    "After computing the distances, we can filter out obviously invalid points. Some validity checks are:\n",
    "\n",
    "1. `control_trajectories`: points are valid if they are close to each other\n",
    "1. `ground_truth_both`: points are valid if they are close to the ground truth in both trajectories\n",
    "1. `ground_truth_either`: points are valid if they are close to the ground truth in either trajectory\n",
    "1. `distance_along_route`: points are valid if they are further along the ground truth trajectory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging\n",
    "\n",
    "After we compare the trajectories and decide which points to retain, we will potentially have two fairly close-by points. We need to figure out how to combine them into one. Potential options are:\n",
    "1. random (requires only control trajectories)\n",
    "1. midpoint (requires only control trajectories)\n",
    "1. closer to ground truth (requires control trajectories and ground truth)\n",
    "1. closer to previous distance along route (requires control trajectories and ground truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate algorithms\n",
    "\n",
    "We combine the components above into the following potential algorithms\n",
    "\n",
    "#### Control trajectory-only comparisons\n",
    "\n",
    "Note that in this case, because we interpolate for different ranges, we need to ensure that we are calculating the diff between the same points and not different points. See `train_bus_ebike_mtv_ucb/berkeley_to_mtv_SF_express_bus/ebike_bikeshare_urban_long_1` for an example. The section started at `2019-07-25T16:36:32.386274-07:00`, but on iOS, we did not get any points until 16:36:41, although we had android points right from 16:36:33. This means that we were matching up points at different timestamps, which caused the distance to be larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_ucb.map()[\"ios\"][\"ucb-sdb-ios-1\"][\"evaluation_ranges\"][1][\"evaluation_trip_ranges\"][2][\"location_df\"].query(\"ts > (1564097792 - 15) & ts < (1564097792 + 15)\").fmt_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_ucb.map()[\"android\"][\"ucb-sdb-android-1\"][\"evaluation_ranges\"][1][\"evaluation_trip_ranges\"][2][\"location_df\"].query(\"ts > (1564097792 - 15) & ts < (1564097792 + 15)\").fmt_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_gt_and_reference(entry, loc_df_label, with_points=False):\n",
    "    curr_map = folium.Map()\n",
    "    # print(\"Using ground truth %s\" % entry[\"ground_truth\"][\"leg\"][\"id\"])\n",
    "    gt_leg_gj = eisd.SpecDetails.get_geojson_for_leg(entry[\"ground_truth\"][\"leg\"])\n",
    "    gt_leg_gj_feature = folium.GeoJson(gt_leg_gj, name=\"ground_truth\")\n",
    "    curr_map.add_child(gt_leg_gj_feature)\n",
    "    if with_points:\n",
    "        gt_leg_gj_points = ezpv.get_point_markers(gt_leg_gj[2], name=\"ground_truth_points\", color=\"green\")\n",
    "        curr_map.add_child(gt_leg_gj_points)\n",
    "    \n",
    "    sensed_location_df = entry[loc_df_label]\n",
    "    # print(\"Adding section for %s with length %s\" % (loc_df_label, len(sensed_location_df)))\n",
    "    if len(sensed_location_df) > 0:\n",
    "        sensed_section_gj = gj.Feature(geometry=gj.LineString(coordinates=list(zip(sensed_location_df.longitude, sensed_location_df.latitude))),\n",
    "                                   properties={\"style\": {\"color\": \"red\"}, \"ts\": list(sensed_location_df.ts)})\n",
    "        sensed_leg_gj_feature = folium.GeoJson(sensed_section_gj, name=\"reference_trajectory\")\n",
    "        curr_map.add_child(sensed_leg_gj_feature)\n",
    "        if with_points:\n",
    "            sensed_leg_gj_points = ezpv.get_point_markers(sensed_section_gj, name=\"reference_points\", color=\"red\", tz=\"America/Los_Angeles\")\n",
    "            curr_map.add_child(sensed_leg_gj_points)\n",
    "    \n",
    "    curr_map.fit_bounds(gt_leg_gj_feature.get_bounds())\n",
    "    folium.LayerControl().add_to(curr_map)\n",
    "    return curr_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 40075017 # circumference of the earth in meters\n",
    "\n",
    "def ref_ct_general(e, b_merge_fn, dist_threshold, tz=\"UTC\"):\n",
    "    new_location_df_a = emr.get_int_aligned_trajectory(e[\"temporal_control\"][\"android\"][\"location_df\"], tz)\n",
    "    new_location_df_i = emr.get_int_aligned_trajectory(e[\"temporal_control\"][\"ios\"][\"location_df\"], tz)\n",
    "    merged_df = pd.merge(new_location_df_a, new_location_df_i, on=\"ts\",\n",
    "        how=\"inner\", suffixes=(\"_a\", \"_i\")).sort_values(by=\"ts\", axis=\"index\")\n",
    "    merged_df[\"t_distance\"] = gpd.GeoSeries(merged_df.geometry_a).distance(gpd.GeoSeries(merged_df.geometry_i)) * (R/360)\n",
    "    filtered_merged_df = merged_df.query(\"t_distance < @dist_threshold\")\n",
    "    print(\"After filtering, retained %d of %d (%s)\" % \n",
    "          (len(filtered_merged_df), max(len(new_location_df_a), len(new_location_df_i)),\n",
    "            (len(filtered_merged_df)/max(len(new_location_df_a), len(new_location_df_i)))))\n",
    "    merge_fn = functools.partial(emr.collapse_inner_join, b_merge_fn=b_merge_fn)\n",
    "    initial_reference_gpdf = gpd.GeoDataFrame(list(filtered_merged_df.apply(merge_fn, axis=1)))\n",
    "    print(initial_reference_gpdf.columns)\n",
    "    if len(initial_reference_gpdf.columns) > 1:\n",
    "        initial_reference_gpdf[\"fmt_time\"] = initial_reference_gpdf.ts.apply(lambda ts: arrow.get(ts).to(tz))\n",
    "        assert len(initial_reference_gpdf[initial_reference_gpdf.latitude.isnull()]) == 0, \"Found %d null entries out of %d total\" % (len(initial_reference_gpdf.latitude.isnull()), len(initial_reference_gpdf))\n",
    "        return initial_reference_gpdf\n",
    "    else:\n",
    "        return gpd.GeoDataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Effect of threshold\n",
    "\n",
    "For the \"good\" trajectories (`easiest` and `backtracking`), the threshold has essentially no effect. The trajectories are so good that the tolerance is less than 10 meters and we retain the entire trajectory. For the other two, even if the coverage is good, the quality of the final trajectory is not that great. For example, consider the offset along Hawthorne Ave in the `weird_android` case and the deviations from the ground truth along BART in the `temporal_zigzags` case.\n",
    "\n",
    "And for the `temporal_zigzags`, at small thresholds, almost nothing matches.\n",
    "\n",
    "Also, in the case where the data was missing from one of the trajectories (i.e. `temporal_zigzags` on `android`), our reference trajectory also ends, leading to poor coverage compared to the alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in deep_dive_dataset.values():\n",
    "    e[\"ct_midpoint_loc_df_10\"] = ref_ct_general(e, emr.b_merge_midpoint,\n",
    "                                               10, tz=\"America/Los_angeles\")\n",
    "    e[\"ct_midpoint_loc_df_25\"] = ref_ct_general(e, emr.b_merge_midpoint,\n",
    "                                               25, tz=\"America/Los_angeles\")\n",
    "    e[\"ct_midpoint_loc_df_50\"] = ref_ct_general(e, emr.b_merge_midpoint,\n",
    "                                               50, tz=\"America/Los_angeles\")\n",
    "    e[\"ct_midpoint_loc_df_100\"] = ref_ct_general(e, emr.b_merge_midpoint,\n",
    "                                               100, tz=\"America/Los_angeles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_maps = bre.Figure()\n",
    "test_candidates = [\"ct_midpoint_loc_df_10\", \"ct_midpoint_loc_df_25\", \"ct_midpoint_loc_df_50\", \"ct_midpoint_loc_df_100\"]\n",
    "nRows = len(test_candidates)\n",
    "for i, c in enumerate(test_candidates):\n",
    "    evaluation_maps.add_child(bre.Html(c))\n",
    "    map_list = [display_gt_and_reference(e, c) for e in deep_dive_dataset.values()]    \n",
    "    for j, curr_map in enumerate(map_list):\n",
    "        evaluation_maps.add_subplot(nRows, 4, 4*i+(j+1)).add_child(curr_map)\n",
    "evaluation_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Effect of merge function\n",
    "\n",
    "Again, for the \"good\" trajectories (`easiest` and `backtracking`), the merge function has essentially no effect. The trajectories are so good that the tolerance is less than 10 meters and using the midpoint versus random versus closest makes no difference.\n",
    "\n",
    "For the other two, choosing the closest point allows us to handle the case where the errors are related primarily to one phone (e.g. in the `weird_android` case). So if the iOS trajectory is good (as determined by a comparison with spatial ground truth), we can use it directly and not have it be polluted by the bad android trajectory.\n",
    "\n",
    "So effectively, with this approach, we are choosing the good points from each of the trajectories and stitching them together to create an overall trajectory. This implies that we cannot use an inner join for the merge, since then we would only retain points that were good in **both** trajectories. We use an outer join instead, and if there are points that are good only in one trajectory, we just select them.\n",
    "\n",
    "Note that this allows us to use much smaller thresholds and still get coverages similar to the higher thresholds while using midpoints\n",
    "\n",
    "However, stitching together points from multiple trajectories is challenging if there are time sychronization issues. While the timestamps for a single phone are guaranteed to be consistent, timestamps across phones are not guaranteed to be consistent, and flip flopping between phones can lead to zigzags in the final trajectories. For example, in the `weird_android` case, see:\n",
    "- the section of Springer between Todd and El Monte,\n",
    "- the intersection of El Camino and Shoreline,\n",
    "- El Monte near Mayer Court"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_gt_general(e, b_merge_fn, dist_threshold,\n",
    "        tz=\"UTC\"):\n",
    "    new_location_df_a = emr.get_int_aligned_trajectory(e[\"temporal_control\"][\"android\"][\"location_df\"], tz)\n",
    "    new_location_df_i = emr.get_int_aligned_trajectory(e[\"temporal_control\"][\"ios\"][\"location_df\"], tz)\n",
    "    emr.fill_gt_linestring(e)\n",
    "    gt_linestring = e[\"ground_truth\"][\"linestring\"]\n",
    "    emr.add_gt_error_projection(new_location_df_a, gt_linestring)\n",
    "    emr.add_gt_error_projection(new_location_df_i, gt_linestring)\n",
    "    filtered_location_df_a = new_location_df_a.query(\"gt_distance < @dist_threshold\")\n",
    "    filtered_location_df_i = new_location_df_i.query(\"gt_distance < @dist_threshold\")\n",
    "    print(\"After filtering, %d of %d (%s) for android and %d of %d (%s) for ios\" % \n",
    "          (len(filtered_location_df_a), len(new_location_df_a), (len(filtered_location_df_a)/len(new_location_df_a)),\n",
    "           len(filtered_location_df_i), len(new_location_df_i), (len(filtered_location_df_i)/len(new_location_df_i))))\n",
    "    merged_df = pd.merge(filtered_location_df_a, filtered_location_df_i, on=\"ts\",\n",
    "        how=\"outer\", suffixes=(\"_a\", \"_i\")).sort_values(by=\"ts\", axis=\"index\")\n",
    "    merge_fn = functools.partial(emr.collapse_outer_join_stateless, b_merge_fn=b_merge_fn)\n",
    "    initial_reference_gpdf = gpd.GeoDataFrame(list(merged_df.apply(merge_fn, axis=1)))\n",
    "    if len(initial_reference_gpdf.columns) > 1:\n",
    "        initial_reference_gpdf[\"fmt_time\"] = initial_reference_gpdf.ts.apply(lambda ts: arrow.get(ts).to(tz))\n",
    "        print(\"After merging, found %d of android %d (%s), ios %d (%s)\" % \n",
    "              (len(initial_reference_gpdf), len(new_location_df_a), (len(initial_reference_gpdf)/len(new_location_df_a)),\n",
    "               len(new_location_df_i), (len(initial_reference_gpdf)/len(new_location_df_i))))\n",
    "        assert len(initial_reference_gpdf[initial_reference_gpdf.latitude.isnull()]) == 0, \"Found %d null entries out of %d total\" % (len(initial_reference_gpdf.latitude.isnull()), len(initial_reference_gpdf))\n",
    "        return initial_reference_gpdf\n",
    "    else:\n",
    "        return gpd.GeoDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in deep_dive_dataset.values():\n",
    "    e[\"ct_closer_gt_dist_10\"] = ref_gt_general(e, emr.b_merge_closer_gt_dist,\n",
    "                                               10, tz=\"America/Los_angeles\")\n",
    "    e[\"ct_closer_gt_dist_25\"] = ref_gt_general(e, emr.b_merge_closer_gt_dist,\n",
    "                                               25, tz=\"America/Los_angeles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluation_maps = bre.Figure()\n",
    "test_candidates = [\"ct_closer_gt_dist_10\", \"ct_closer_gt_dist_25\", \"ct_midpoint_loc_df_50\"]\n",
    "nRows = len(test_candidates)\n",
    "for i, c in enumerate(test_candidates):\n",
    "    evaluation_maps.add_child(bre.Html(c))\n",
    "    map_list = [display_gt_and_reference(e, c) for e in deep_dive_dataset.values()]    \n",
    "    for j, curr_map in enumerate(map_list):\n",
    "        evaluation_maps.add_subplot(nRows, 4, 4*i+(j+1)).add_child(curr_map)\n",
    "evaluation_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Incorporating _forward motion_\n",
    "\n",
    "We can handle the time synchronization issues by ensuring that travel only occurs \"forward\" along the ground truth trajectory. We can determine the distance travelled so far as a projection of the control trajectory along the ground truth and only incorporate points that make forward progress. Note that this requires the use of a global variable to track the distance so far, and the merging function is no longer stateless.\n",
    "\n",
    "And, as feared, in the place where there is a genuine backtrack (`backtracking` use case at Market and Devine), we trim some of the backtrack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(emr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_travel_forward(e, dist_threshold, tz=\"UTC\"):\n",
    "    # This function needs a global variable\n",
    "    emr.distance_so_far = 0\n",
    "    new_location_df_a = emr.get_int_aligned_trajectory(e[\"temporal_control\"][\"android\"][\"location_df\"], tz)\n",
    "    new_location_df_i = emr.get_int_aligned_trajectory(e[\"temporal_control\"][\"ios\"][\"location_df\"], tz)\n",
    "    emr.fill_gt_linestring(e)\n",
    "    gt_linestring = e[\"ground_truth\"][\"linestring\"]\n",
    "    emr.add_gt_error_projection(new_location_df_a, gt_linestring)\n",
    "    emr.add_gt_error_projection(new_location_df_i, gt_linestring)\n",
    "    new_location_df_a[\"gt_cum_proj\"] = new_location_df_a.gt_projection.cumsum()\n",
    "    new_location_df_i[\"gt_cum_proj\"] = new_location_df_i.gt_projection.cumsum()\n",
    "    filtered_location_df_a = new_location_df_a.query(\"gt_distance < @dist_threshold\")\n",
    "    filtered_location_df_i = new_location_df_i.query(\"gt_distance < @dist_threshold\")\n",
    "    print(\"After filtering, %d of %d (%s) for android and %d of %d (%s) for ios\" % \n",
    "          (len(filtered_location_df_a), len(new_location_df_a), (len(filtered_location_df_a)/len(new_location_df_a)),\n",
    "           len(filtered_location_df_i), len(new_location_df_i), (len(filtered_location_df_i)/len(new_location_df_i))))\n",
    "    merged_df = pd.merge(filtered_location_df_a, filtered_location_df_i, on=\"ts\",\n",
    "        how=\"outer\", suffixes=(\"_a\", \"_i\")).sort_values(by=\"ts\", axis=\"index\")\n",
    "    merge_fn = functools.partial(emr.collapse_outer_join_dist_so_far, more_details_fn = None)\n",
    "    initial_reference_gpdf = gpd.GeoDataFrame(list(merged_df.apply(merge_fn, axis=1)))\n",
    "    if len(initial_reference_gpdf.columns) > 1:\n",
    "        initial_reference_gpdf[\"fmt_time\"] = initial_reference_gpdf.ts.apply(lambda ts: arrow.get(ts).to(tz))\n",
    "        reference_gpdf = initial_reference_gpdf[initial_reference_gpdf.latitude.notnull()]\n",
    "        print(\"After merging, found %d / %d of android %d (%s), ios %d (%s)\" % \n",
    "              (len(reference_gpdf), len(initial_reference_gpdf), len(new_location_df_a), (len(reference_gpdf)/len(new_location_df_a)),\n",
    "               len(new_location_df_i), (len(reference_gpdf)/len(new_location_df_i))))\n",
    "        assert len(reference_gpdf[reference_gpdf.latitude.isnull()]) == 0, \"Found %d null entries out of %d total\" % (len(reference_gpdf[reference_gpdf.latitude.isnull()]), len(initial_reference_gpdf))\n",
    "        return reference_gpdf\n",
    "    else:\n",
    "        return gpd.GeoDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in deep_dive_dataset.values():\n",
    "    e[\"travel_forward_10\"] = ref_travel_forward(e, 10, tz=\"America/Los_angeles\")\n",
    "    e[\"travel_forward_25\"] = ref_travel_forward(e, 25, tz=\"America/Los_angeles\")\n",
    "    e[\"travel_forward_50\"] = ref_travel_forward(e, 50, tz=\"America/Los_angeles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_maps = bre.Figure()\n",
    "test_candidates = [\"travel_forward_10\", \"travel_forward_25\", \"travel_forward_50\"]\n",
    "nRows = len(test_candidates)\n",
    "for i, c in enumerate(test_candidates):\n",
    "    evaluation_maps.add_child(bre.Html(c))\n",
    "    map_list = [display_gt_and_reference(e, c) for e in deep_dive_dataset.values()]    \n",
    "    for j, curr_map in enumerate(map_list):\n",
    "        evaluation_maps.add_subplot(nRows, 4, 4*i+(j+1)).add_child(curr_map)\n",
    "evaluation_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the coverage of reference trajectories\n",
    "\n",
    "- The `ct_midpoint` methods do not require ground truth, do not generate zigzags and work well with backtracking. Their main drawback is that they work well only for very high quality trajectories.\n",
    "- The `forward_motion` method works well even with poor quality trajectories and gives great coverage but does not handle backtracking\n",
    "\n",
    "An ensemble of the two is likely to be a successful method overall. Let's estimate how this will work statistically for the trajectories from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_linestring(gt_leg):\n",
    "    \"\"\"\n",
    "    Get lat-long corrdinates in ground truth\n",
    "    \"\"\"\n",
    "    if 'route_coords' in gt_leg:\n",
    "        coords = gt_leg['route_coords']['geometry']['coordinates']\n",
    "    else:\n",
    "        coords = []\n",
    "    return shp.geometry.LineString(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reference_trajectory_tree(pv):\n",
    "    ref_tree = {}\n",
    "    # This is a GeoSeries\n",
    "    # polygons = get_polygons(pv)\n",
    "    \n",
    "    for phone_os, phone_map in pv.map().items():\n",
    "        for phone_label, phone_detail_map in phone_map.items():\n",
    "            for (r_idx, r) in enumerate(phone_detail_map[\"evaluation_ranges\"]):\n",
    "                if r[\"eval_role_base\"] != \"accuracy_control\":\n",
    "                    continue\n",
    "                for (tr_idx, tr) in enumerate(r[\"evaluation_trip_ranges\"]):\n",
    "                    ref_travel_forward\n",
    "                    for (sr_idx, sr) in enumerate(tr[\"evaluation_section_ranges\"]):\n",
    "                        # This is a Shapely LineString\n",
    "                        section_gt_leg = pv.spec_details.get_ground_truth_for_leg(tr[\"trip_id_base\"], sr[\"trip_id_base\"])\n",
    "                        section_gt_points = get_gt_linestring(section_gt_leg)\n",
    "                        if section_gt_points.is_empty:\n",
    "                            print(\"No ground truth route for %s %s, must be polygon, skipping...\" % (tr[\"trip_id_base\"], sr[\"trip_id_base\"]))\n",
    "                            assert section_gt_leg[\"type\"] != \"TRAVEL\", \"For %s, %s, %s, %s, %s found type %s\" % (phone_os, phone_label, r_idx, tr_idx, sr_idx, section_gt_leg[\"type\"])\n",
    "                            continue\n",
    "                        if len(sr['location_df']) == 0:\n",
    "                            print(\"No sensed locations found, role = %s skipping...\" % (r[\"eval_role_base\"]))\n",
    "                            # assert r[\"eval_role_base\"] == \"power_control\", \"Found no locations for %s, %s, %s, %s, %s\" % (phone_os, phone_label, r_idx, tr_idx, sr_idx)\n",
    "                            continue\n",
    "                        \n",
    "                        sec_name = tr[\"trip_id_base\"] + \"/\" + sr[\"trip_id_base\"] + \"_\" + str(r_idx)\n",
    "                        if sec_name not in ref_tree:\n",
    "                            ref_tree[sec_name] = {\n",
    "                                \"trip_id\": tr[\"trip_id_base\"],\n",
    "                                \"section_id\": sr[\"trip_id_base\"],\n",
    "                                \"run\": r_idx,\n",
    "                                \"ground_truth\": {\n",
    "                                    \"leg\": section_gt_leg,\n",
    "                                    \"linestring\": section_gt_points\n",
    "                                }\n",
    "                            }\n",
    "                        \n",
    "                        assert sec_name in ref_tree\n",
    "                        e = ref_tree[sec_name]\n",
    "                        # This is a GeoDataFrame\n",
    "                        # section_measured_points = get_travel_trajectory(sr['location_df'], polygons)\n",
    "                        section_measured_points = sr[\"location_df\"]\n",
    "                        if \"temporal_control\" not in e:\n",
    "                            e[\"temporal_control\"] = {}\n",
    "                            e[\"start_ts\"] = sr[\"start_ts\"]\n",
    "                            e[\"end_ts\"] = sr[\"end_ts\"]\n",
    "                        e[\"temporal_control\"][phone_os] = sr\n",
    "    return ref_tree\n",
    "                        \n",
    "def fill_ref_tree_entry(e):\n",
    "    print(\"Considering entry %s %s %s\" % (e[\"trip_id\"], e[\"section_id\"], e[\"run\"]))\n",
    "    curr_tz = \"America/Los_angeles\"\n",
    "    assert \"android\" in e[\"temporal_control\"] and \"ios\" in e[\"temporal_control\"]\n",
    "    e[\"travel_forward_25\"] = ref_travel_forward(e, 25, tz=curr_tz)\n",
    "    e[\"travel_forward_50\"] = ref_travel_forward(e, 50, tz=curr_tz)\n",
    "    e[\"ct_midpoint_25\"] = ref_ct_general(e, emr.b_merge_midpoint, 25, tz=curr_tz)\n",
    "    e[\"ct_midpoint_50\"] = ref_ct_general(e, emr.b_merge_midpoint, 50, tz=curr_tz)\n",
    "\n",
    "def get_coverage(pv, ref_tree):\n",
    "    spatial_error_list = []\n",
    "    for e in ref_tree.values():\n",
    "        # we resample every second, so full coverage would involve 1 entry for each second in the section\n",
    "        # If we have less than that, the length of the dataframe will be less\n",
    "        coverage = lambda df, sr: len(df)/(sr[\"end_ts\"] - sr[\"start_ts\"])\n",
    "        spatial_error_base = {\"timeline\": pv.spec_details.curr_spec[\"id\"],\n",
    "                         \"trip_id\": e[\"trip_id\"], \"section_id\": e[\"section_id\"], \"run\": e[\"run\"]}\n",
    "        \n",
    "        for result in [\"travel_forward_25\", \"travel_forward_50\", \"ct_midpoint_25\", \"ct_midpoint_50\"]:\n",
    "            spatial_error = copy.copy(spatial_error_base)\n",
    "            spatial_error[\"algo\"] = result\n",
    "            spatial_error[\"coverage\"] = coverage(e[result], e)\n",
    "            spatial_error_list.append(spatial_error)\n",
    "    return spatial_error_list\n",
    "\n",
    "def get_reference_coverage(pv, ref_tree_root):\n",
    "    ref_tree = get_reference_trajectory_tree(pv)\n",
    "    ref_tree_root[pv.spec_details.curr_spec[\"id\"]] = ref_tree\n",
    "    [fill_ref_tree_entry(e) for e in ref_tree.values()]\n",
    "    return get_coverage(pv, ref_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_tree = {}\n",
    "coverage_list = []\n",
    "coverage_list.extend(get_reference_coverage(pv_la, ref_tree))\n",
    "coverage_list.extend(get_reference_coverage(pv_sj, ref_tree))\n",
    "coverage_list.extend(get_reference_coverage(pv_ucb, ref_tree))\n",
    "\n",
    "coverage_df = pd.DataFrame(coverage_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline_list = [\"car_scooter_brex_san_jose\", \"unimodal_trip_car_bike_mtv_la\"]\n",
    "test_candidates = [\"ct_midpoint_25\", \"ct_midpoint_50\", \"travel_forward_25\", \"travel_forward_50\"]\n",
    "for i, tl in enumerate(timeline_list):\n",
    "    unique_sections = pd.Series(coverage_df.query(\"timeline == @tl\").section_id.unique())\n",
    "    ifig, ax_array = plt.subplots(nrows=1,ncols=len(unique_sections),figsize=(12,3), sharex=False, sharey=True)\n",
    "    for j, sid in enumerate(unique_sections):\n",
    "        coverage_df.query(\"timeline == @tl & section_id == @sid\").boxplot(ax = ax_array[j], column=[\"coverage\"], by=[\"algo\"], showbox=False, whis=\"range\")\n",
    "        ax_array[j].set_title(sid)\n",
    "        ax_array[j].set_xticklabels([\"ctm_25\", \"ctm_50\", \"tf_25\", \"tf_50\"])\n",
    "        ax_array[j].set_xlabel(\"\")\n",
    "    ifig.suptitle(tl, y=1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_timeline = \"train_bus_ebike_mtv_ucb\"\n",
    "test_candidates = [\"ct_midpoint_25\", \"ct_midpoint_50\", \"travel_forward_25\", \"travel_forward_50\"]\n",
    "\n",
    "unique_sections = pd.Series(coverage_df.query(\"timeline == @long_timeline\").section_id.unique())\n",
    "nRows = len(unique_sections)/3 + 1\n",
    "ifig = plt.Figure(figsize=(15,15))\n",
    "ax_list = []\n",
    "first_ax = None\n",
    "for j, sid in enumerate(unique_sections):\n",
    "    if first_ax is None:\n",
    "        curr_ax = ifig.add_subplot(nRows, 3, j+1)\n",
    "        first_ax = curr_ax        \n",
    "    else:\n",
    "        curr_ax = ifig.add_subplot(nRows, 3, j+1, sharex=first_ax, sharey=first_ax)\n",
    "    ax_list.append(curr_ax)\n",
    "    coverage_df.query(\"timeline == @long_timeline & section_id == @sid\").boxplot(ax = curr_ax, column=[\"coverage\"], by=[\"algo\"], showbox=False, whis=\"range\")\n",
    "    curr_ax.set_title(sid)\n",
    "    curr_ax.set_xlabel(\"\")\n",
    "    curr_ax.tick_params(labelbottom=False)\n",
    "    curr_ax.tick_params(labelleft=False)\n",
    "    \n",
    "for i in range(1,4):\n",
    "    ax_list[-i].tick_params(labelbottom=True)\n",
    "    ax_list[-i].set_xticklabels([\"ctm_25\", \"ctm_50\", \"tf_25\", \"tf_50\"])\n",
    "    \n",
    "for i in range(0,len(ax_list),3):\n",
    "    print(\"Displaying y tick labels for %s\" % i)\n",
    "    ax_list[i].tick_params(labelleft=True)\n",
    "\n",
    "ifig.suptitle(long_timeline)\n",
    "ifig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = coverage_df.query(\"section_id == 'freeway_driving_weekday' & (algo == 'travel_forward_25' | algo == 'ct_midpoint_25')\"); outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier verification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### travel_forward_25 \t0.565114 \t1 \tcommuter_rail_with_tunnels \ttrain_bus_ebike_mtv_ucb \tberkeley_to_mtv_SF_express_bus\n",
    "\n",
    "In this case, the android trajectory is correct, but the iOS trajectory is all over the place, with lots of zigzags, gaps and other wierdnesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sel_name = \"berkeley_to_mtv_SF_express_bus/commuter_rail_with_tunnels_1\"\n",
    "e = ref_tree[\"train_bus_ebike_mtv_ucb\"][sel_name]\n",
    "fig = bre.Figure()\n",
    "fig.add_subplot(1,3,1).add_child(display_gt_and_reference(e, \"travel_forward_25\", with_points=True))\n",
    "fig.add_subplot(1,3,2).add_child(display_gt_and_reference(e, \"ct_midpoint_25\", with_points=True))\n",
    "fig.add_subplot(1,3,3).add_child(display_gt_and_controls(e, \"location_df\", with_points=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### travel_forward_25 \t0.166812 \t2 \twalk to the bikeshare location \ttrain_bus_ebike_mtv_ucb \tberkeley_to_mtv_SF_express_bus\n",
    "\n",
    "This is because of walking around to find the bicycle, and the fact that we don't strip out the points within the polygons any more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_name = \"berkeley_to_mtv_SF_express_bus/walk to the bikeshare location_2\"\n",
    "e = ref_tree[\"train_bus_ebike_mtv_ucb\"][sel_name]\n",
    "fig = bre.Figure()\n",
    "fig.add_subplot(1,2,1).add_child(display_gt_and_reference(e, \"travel_forward_25\", with_points=True))\n",
    "fig.add_subplot(1,2,2).add_child(display_gt_and_reference(e, \"ct_midpoint_25\", with_points=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### travel_forward_25 \t0.836886 \t4 \twalk_back_from_bus \tcar_scooter_brex_san_jose \tbus trip with e-scooter access\n",
    "\n",
    "Looks fine; unsure why this is so low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_name = \"bus trip with e-scooter access/walk_back_from_bus_4\"\n",
    "e = ref_tree[\"car_scooter_brex_san_jose\"][sel_name]\n",
    "fig = bre.Figure()\n",
    "fig.add_subplot(1,2,1).add_child(display_gt_and_reference(e, \"travel_forward_25\", with_points=True))\n",
    "fig.add_subplot(1,2,2).add_child(display_gt_and_reference(e, \"ct_midpoint_25\", with_points=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### travel_forward_25 \t0.276315 \t2 \tlight_rail_below_above_ground \ttrain_bus_ebike_mtv_ucb \tberkeley_to_mtv_SF_express_bus\n",
    "\n",
    "In this case, there is a time mismatch, so adjacent points are separated by ~ 6 seconds.\n",
    "e.g. at around Embarcadero and Bryant, the android point is `2019-07-26 18:15:18` while the iOS point is `2019-07-26 18:15:24`. The android point that matches `2019-07-26 18:15:24` is halfway down the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_name = \"berkeley_to_mtv_SF_express_bus/light_rail_below_above_ground_2\"\n",
    "e = ref_tree[\"train_bus_ebike_mtv_ucb\"][sel_name]\n",
    "fig = bre.Figure()\n",
    "fig.add_subplot(1,3,1).add_child(display_gt_and_reference(e, \"travel_forward_25\", with_points=True))\n",
    "fig.add_subplot(1,3,2).add_child(display_gt_and_reference(e, \"ct_midpoint_25\", with_points=True))\n",
    "fig.add_subplot(1,3,3).add_child(display_gt_and_controls(e, \"location_df\", with_points=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# travel_forward_25 \t0.350489 \t1 \twalk_to_bus \ttrain_bus_ebike_mtv_ucb \tmtv_to_berkeley_sf_bart\n",
    "sel_name = \"mtv_to_berkeley_sf_bart/walk_to_bus_1\"\n",
    "e = ref_tree[\"train_bus_ebike_mtv_ucb\"][sel_name]\n",
    "fig = bre.Figure()\n",
    "fig.add_subplot(1,3,1).add_child(display_gt_and_reference(e, \"travel_forward_25\", with_points=True))\n",
    "fig.add_subplot(1,3,2).add_child(display_gt_and_reference(e, \"ct_midpoint_25\", with_points=True))\n",
    "fig.add_subplot(1,3,3).add_child(display_gt_and_controls(e, \"location_df\", with_points=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# travel_forward_25 \t0.845444 \t0 \tfreeway_driving_weekday \tcar_scooter_brex_san_jose \tfreeway_driving_weekday\n",
    "sel_name = \"freeway_driving_weekday/freeway_driving_weekday_0\"\n",
    "e = ref_tree[\"car_scooter_brex_san_jose\"][sel_name]\n",
    "fig = bre.Figure()\n",
    "fig.add_subplot(1,3,1).add_child(display_gt_and_reference(e, \"travel_forward_25\", with_points=True))\n",
    "fig.add_subplot(1,3,2).add_child(display_gt_and_reference(e, \"ct_midpoint_25\", with_points=True))\n",
    "fig.add_subplot(1,3,3).add_child(display_gt_and_controls(e, \"location_df\", with_points=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroid(curr_map):\n",
    "    bbox = []\n",
    "    [bbox.extend(reversed(p)) for p in curr_map.get_bounds()]\n",
    "    return shp.geometry.box(*bbox).centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
